{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavan-charan/Fake-news-detection-QML/blob/main/Quantum_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXdEqvjIr9yz"
      },
      "outputs": [],
      "source": [
        "# This cell mounts Google Drive to the Colab environment,\n",
        "# allowing access to files stored in the user's Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfg0BlGMsCJv"
      },
      "outputs": [],
      "source": [
        "# This cell installs the required Python libraries for the project.\n",
        "# - pennylane & pennylane-lightning: for quantum machine learning.\n",
        "# - scikit-learn: for classical machine learning and data processing.\n",
        "# - pandas, numpy, openpyxl: for data manipulation.\n",
        "# - seaborn: for plotting.\n",
        "!pip install pennylane pennylane-lightning scikit-learn pandas numpy openpyxl seaborn --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-section"
      },
      "source": [
        "# Fake News Detection using QML\n",
        "\n",
        "This notebook demonstrates a project on fake news detection using both classical and quantum machine learning techniques. The goal is to compare the performance of these methods on a real-world dataset of COVID-19 related tweets. The notebook is structured as follows:\n",
        "1. **Data Preprocessing**: Loading, cleaning, and preparing the text data.\n",
        "2. **Classical Models**: Training and evaluating classical machine learning models (Linear SVC and RBF-SVM) as a baseline.\n",
        "3. **Quantum-Enhanced Model (QSVC)**: Implementing and evaluating a quantum-enhanced Support Vector Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYa5_p1RsPh1"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for the project.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, time, gc\n",
        "\n",
        "# Scikit-learn utilities for feature extraction, dimensionality reduction, preprocessing, and model evaluation.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# Libraries for plotting and visualization.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# PennyLane for quantum machine learning.\n",
        "import pennylane as qml\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    \"\"\"Cleans a string by converting to lowercase and removing URLs, mentions, hashtags, and non-alphanumeric characters.\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)  # Remove URLs\n",
        "    s = re.sub(r\"@\\w+\", \" \", s)              # Remove mentions\n",
        "    s = re.sub(r\"#\", \" \", s)                  # Remove hashtags\n",
        "    s = re.sub(r\"[^a-z0-9'\\s]\", \" \", s)       # Remove non-alphanumeric characters\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()         # Remove extra spaces\n",
        "    return s\n",
        "\n",
        "def timer():\n",
        "    \"\"\"A simple timer function to measure the execution time of code blocks.\"\"\"\n",
        "    t=[time.time()]\n",
        "    return lambda msg=\"\": (print(f\"{msg}: {time.time()-t[0]:.2f}s\"), t.__setitem__(0,time.time()))\n",
        "\n",
        "def report(name, y_true, y_pred):\n",
        "    \"\"\"Calculates and prints the accuracy and F1-score for a given model's predictions.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    print(f\"{name} â€” Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
        "    return acc, f1\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
        "    \"\"\"Plots a confusion matrix to visualize the performance of a classification model.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-preprocessing-section"
      },
      "source": [
        "## 1. Data Preprocessing\n",
        "\n",
        "In this section, we load the dataset, clean the text data, and prepare it for the models. The main steps include:\n",
        "- Loading the training, validation, and test sets.\n",
        "- Cleaning the text by removing URLs, mentions, and special characters.\n",
        "- Encoding the labels ('real' and 'fake') into numerical format.\n",
        "- Applying the TF-IDF vectorizer to convert text into numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcDR7KcCsfeg"
      },
      "outputs": [],
      "source": [
        "# Load the training, validation, and test datasets from Excel files.\n",
        "train_df = pd.read_excel(\"/content/drive/MyDrive/CovidDataset/Constraint_English_Train.xlsx\")\n",
        "val_df   = pd.read_excel(\"/content/drive/MyDrive/CovidDataset/Constraint_English_Val.xlsx\")\n",
        "test_df  = pd.read_excel(\"/content/drive/MyDrive/CovidDataset/english_test_with_labels.xlsx\")\n",
        "\n",
        "# Clean the 'tweet' column in each DataFrame.\n",
        "for df in (train_df, val_df, test_df):\n",
        "    df[\"tweet\"] = df[\"tweet\"].astype(str).map(clean_text)\n",
        "\n",
        "# Encode the 'label' column into numerical format (e.g., 'real' -> 1, 'fake' -> 0).\n",
        "le = LabelEncoder()\n",
        "le.fit(pd.concat([train_df[\"label\"], val_df[\"label\"], test_df[\"label\"]]))\n",
        "\n",
        "# Transform the labels for each dataset.\n",
        "y_train = le.transform(train_df[\"label\"])\n",
        "y_val   = le.transform(val_df[\"label\"])\n",
        "y_test  = le.transform(test_df[\"label\"])\n",
        "\n",
        "# Create lists of the cleaned tweet text for each dataset.\n",
        "X_train_text = train_df[\"tweet\"].tolist()\n",
        "X_val_text   = val_df[\"tweet\"].tolist()\n",
        "X_test_text  = test_df[\"tweet\"].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbJsuTGWsnG7"
      },
      "outputs": [],
      "source": [
        "# Initialize the TF-IDF vectorizer to convert text into numerical features.\n",
        "# - ngram_range=(1,2): considers both single words and pairs of words.\n",
        "# - min_df=2: ignores terms that appear in fewer than 2 documents.\n",
        "# - max_features=5000: limits the vocabulary size to the top 5000 terms.\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=5000)\n",
        "\n",
        "# Fit the vectorizer on the training data and transform the training, validation, and test data.\n",
        "Xtr_tfidf = tfidf.fit_transform(X_train_text)\n",
        "Xv_tfidf  = tfidf.transform(X_val_text)\n",
        "Xt_tfidf  = tfidf.transform(X_test_text)\n",
        "\n",
        "# Print the shapes of the resulting TF-IDF matrices.\n",
        "print(\"TF-IDF shapes:\", Xtr_tfidf.shape, Xv_tfidf.shape, Xt_tfidf.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classical-models-section"
      },
      "source": [
        "## 2. Classical Models\n",
        "\n",
        "We first train and evaluate classical machine learning models as a baseline for comparison. The models used are:\n",
        "- **Linear Support Vector Classifier (SVC)**\n",
        "- **SVC with a Radial Basis Function (RBF) kernel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nAIc3OTYs2qL"
      },
      "outputs": [],
      "source": [
        "# Initialize and train a Linear Support Vector Classifier.\n",
        "clf = LinearSVC()\n",
        "clf.fit(Xtr_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the validation and test sets.\n",
        "pred_val = clf.predict(Xv_tfidf)\n",
        "pred_test = clf.predict(Xt_tfidf)\n",
        "\n",
        "# Print the classification report and plot the confusion matrix for the validation set.\n",
        "print(\"\\n=== Linear SVC Validation Report ===\")\n",
        "print(classification_report(y_val, pred_val, target_names=le.classes_))\n",
        "plot_confusion_matrix(y_val, pred_val, le.classes_, \"Linear SVC - Validation\")\n",
        "\n",
        "# Print the classification report and plot the confusion matrix for the test set.\n",
        "print(\"\\n=== Linear SVC Test Report ===\")\n",
        "print(classification_report(y_test, pred_test, target_names=le.classes_))\n",
        "plot_confusion_matrix(y_test, pred_test, le.classes_, \"Linear SVC - Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the dimensionality of the TF-IDF features using Truncated SVD.\n",
        "svd = TruncatedSVD(n_components=10, random_state=42)\n",
        "Ztr = svd.fit_transform(Xtr_tfidf)\n",
        "Zv  = svd.transform(Xv_tfidf)\n",
        "Zt  = svd.transform(Xt_tfidf)\n",
        "\n",
        "# Initialize and train an SVC with an RBF kernel on the reduced features.\n",
        "rbf = SVC(kernel=\"rbf\", C=2.0)\n",
        "rbf.fit(Ztr, y_train)\n",
        "\n",
        "# Make predictions on the validation and test sets.\n",
        "pred_val = rbf.predict(Zv)\n",
        "pred_test = rbf.predict(Zt)\n",
        "\n",
        "# Print the classification report and plot the confusion matrix for the validation set.\n",
        "print(\"\\n=== RBF-SVM Validation Report ===\")\n",
        "print(classification_report(y_val, pred_val, target_names=le.classes_))\n",
        "plot_confusion_matrix(y_val, pred_val, le.classes_, \"RBF SVM - Validation\")\n",
        "\n",
        "# Print the classification report and plot the confusion matrix for the test set.\n",
        "print(\"\\n=== RBF-SVM Test Report ===\")\n",
        "print(classification_report(y_test, pred_test, target_names=le.classes_))\n",
        "plot_confusion_matrix(y_test, pred_test, le.classes_, \"RBF SVM - Test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-7spf6HWDJEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quantum-model-section"
      },
      "source": [
        "## 3. Quantum-Enhanced Model (QSVC)\n",
        "\n",
        "Now, we implement a quantum-enhanced version of the Support Vector Classifier (QSVC). This involves:\n",
        "- Reducing the dimensionality of the data using Truncated SVD.\n",
        "- Scaling the features to be used in the quantum circuit.\n",
        "- Defining a quantum feature map to encode the data into a quantum state.\n",
        "- Creating a quantum kernel to compute the similarity between data points in the quantum feature space.\n",
        "- Training an SVC with the precomputed quantum kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Et2RV9ui9_"
      },
      "outputs": [],
      "source": [
        "# Reduce the dimensionality of the TF-IDF features to 10 components using Truncated SVD.\n",
        "# This is necessary to match the number of qubits in the quantum circuit.\n",
        "svd = TruncatedSVD(n_components=10, random_state=42)\n",
        "Ztr = svd.fit_transform(Xtr_tfidf) # Fit and transform the training data.\n",
        "Zv  = svd.transform(Xv_tfidf)    # Transform the validation data.\n",
        "Zt  = svd.transform(Xt_tfidf)    # Transform the test data.\n",
        "\n",
        "# Print the shape of the transformed training data to confirm the dimensionality reduction.\n",
        "print(\"Ztr shape:\", Ztr.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhr16rGuumji"
      },
      "outputs": [],
      "source": [
        "# Define the number of training samples for the quantum model.\n",
        "TRAIN_Q = 2500\n",
        "\n",
        "# Scale the features to a range of [0, 2*pi] to be used as angles in the quantum circuit.\n",
        "scaler = MinMaxScaler(feature_range=(0, 2*np.pi))\n",
        "Xtr_s = scaler.fit_transform(Ztr)\n",
        "Xv_s  = scaler.transform(Zv)\n",
        "Xt_s  = scaler.transform(Zt)\n",
        "\n",
        "# Function to cap the number of samples to be used.\n",
        "def cap(X, y, n):\n",
        "    idx = np.random.choice(len(X), n, replace=True)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "# Create the final training, validation, and test sets for the quantum model.\n",
        "Xtr_q, ytr_q = cap(Xtr_s, y_train, TRAIN_Q)\n",
        "Xv_q, yv_q   = Xv_s, y_val\n",
        "Xt_q, yt_q   = Xt_s, y_test\n",
        "\n",
        "print(\"Quantum train size =\", len(Xtr_q))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swk2AbKmrVDr"
      },
      "outputs": [],
      "source": [
        "# Set the number of qubits to match the number of features (10).\n",
        "n_qubits = 10\n",
        "\n",
        "# Set up the quantum device, using the high-performance 'lightning.qubit' simulator if available.\n",
        "try:\n",
        "    dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
        "    print(\"Using lightning.qubit backend\")\n",
        "except:\n",
        "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "    print(\"Using default.qubit backend\")\n",
        "\n",
        "# Number of repetitions for the feature map circuit.\n",
        "REPS = 3\n",
        "\n",
        "def feature_map(x):\n",
        "    \"\"\"Quantum feature map to encode classical data into a quantum state.\"\"\"\n",
        "    for r in range(REPS):\n",
        "        # Apply single-qubit rotations based on the input features.\n",
        "        for i in range(n_qubits):\n",
        "            qml.RY(x[i], wires=i)\n",
        "            qml.RZ(x[i]**2, wires=i)\n",
        "        # Apply entangling CZ gates between adjacent qubits.\n",
        "        for i in range(n_qubits - 1):\n",
        "            qml.CZ(wires=[i, i+1])\n",
        "\n",
        "# Define the quantum node that returns the quantum state.\n",
        "@qml.qnode(dev)\n",
        "def quantum_state(x):\n",
        "    \"\"\"Quantum circuit to generate the quantum state for a given input feature vector.\"\"\"\n",
        "    feature_map(x)\n",
        "    return qml.state()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg07c4wfra_a"
      },
      "outputs": [],
      "source": [
        "# Cache to store computed quantum states to avoid redundant calculations.\n",
        "state_cache = {}\n",
        "\n",
        "def get_state(x_tuple):\n",
        "    \"\"\"Retrieves the quantum state from the cache or computes it if not already present.\"\"\"\n",
        "    if x_tuple not in state_cache:\n",
        "        state_cache[x_tuple] = quantum_state(np.array(x_tuple))\n",
        "    return state_cache[x_tuple]\n",
        "\n",
        "def kernel_matrix(A, B):\n",
        "    \"\"\"Computes the quantum kernel matrix between two sets of data points.\"\"\"\n",
        "    NA, NB = len(A), len(B)\n",
        "    K = np.zeros((NA, NB))\n",
        "    for i in range(NA):\n",
        "        a_tuple = tuple(A[i])\n",
        "        psi_a = get_state(a_tuple)\n",
        "        for j in range(NB):\n",
        "            b_tuple = tuple(B[j])\n",
        "            psi_b = get_state(b_tuple)\n",
        "            # The kernel entry is the squared overlap of the quantum states.\n",
        "            K[i, j] = float(np.abs(np.vdot(psi_a, psi_b))**2)\n",
        "        if (i+1) % 50 == 0:\n",
        "            print(f\"Built {i+1}/{NA} rows\")\n",
        "    return K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u_bVfWTsrlwL"
      },
      "outputs": [],
      "source": [
        "# Compute the training kernel matrix.\n",
        "print(\"Building training kernel...\")\n",
        "K_train = kernel_matrix(Xtr_q, Xtr_q)\n",
        "\n",
        "# Train the QSVC using the precomputed training kernel.\n",
        "print(\"Training QSVC...\")\n",
        "qsvc = SVC(kernel=\"precomputed\", C=2.0)\n",
        "qsvc.fit(K_train, ytr_q)\n",
        "\n",
        "# Compute the validation and test kernel matrices.\n",
        "print(\"Building validation kernel...\")\n",
        "K_val = kernel_matrix(Xv_q, Xtr_q)\n",
        "\n",
        "print(\"Building test kernel...\")\n",
        "K_test = kernel_matrix(Xt_q, Xtr_q)\n",
        "\n",
        "# Make predictions on the validation and test sets.\n",
        "pred_val_q = qsvc.predict(K_val)\n",
        "pred_test_q = qsvc.predict(K_test)\n",
        "\n",
        "# Print the classification reports and plot the confusion matrices for the QSVC.\n",
        "print(\"\\nQuantum QSVC Results\")\n",
        "print(\"\\nValidation Report:\")\n",
        "print(classification_report(yv_q, pred_val_q, target_names=le.classes_))\n",
        "plot_confusion_matrix(yv_q, pred_val_q, le.classes_, \"Quantum QSVC - Val\")\n",
        "\n",
        "print(\"\\nTest Report:\")\n",
        "print(classification_report(yt_q, pred_test_q, target_names=le.classes_))\n",
        "plot_confusion_matrix(yt_q, pred_test_q, le.classes_, \"Quantum QSVC - Test\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMVlTKSHOW6/+mUL06w1cwm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}